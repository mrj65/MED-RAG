# MED-RAG
The project focuses on creating a specialized RAG workflow designed for healthcare, specifically to find relative publications based on the query and providing response based on the extracted literature content. 
You can test it out on the hosted huggingface space : https://huggingface.co/spaces/Michaelj1/MedQA-BI.
Keep in mind as its a free hosting service the inference may take upwards to 5min.

# User Interface 
![image](https://github.com/user-attachments/assets/9ac5e5be-3694-4e58-818d-a1a9205b91cf)

The user interface based on Gradio system is designed to answer medical literature-related questions, providing:

Question Input: A prominently placed textbox where users can type their medical queries (e.g., "What are the latest treatments for diabetes?")
Answer Output: A clearly formatted textbox that displays the system-generated response based on the retrieved medical literature
Sources: A section listing the relevant articles used to generate the answer. It includes:
PMIDs (PubMed IDs): Unique identifiers for the referenced studies
Relevance Scores: Numerical scores indicating the relevance of each document to the query
Hyperlinks: Direct links to the articles on PubMed for further reading
Context: A detailed view showing the retrieved document's metadata, including:
Titles
Publication Dates
Abstracts for user reference, providing a deeper understanding of the content behind the sources
Embedding Visualization: A graphical representation showing the alignment and relevance of embeddings between the user query and the top retrieved documents. The heatmap or color-coded bars visually indicate how closely the documents relate to the query

#  RAG Implementation and Integration Workflow
![image](https://github.com/user-attachments/assets/b1a1ec5b-5ef1-438f-b55b-25780c59b70d)

The system uses a pretrained model called ncbi/MedCPT-Query-Encoder to create high-dimensional embeddings for user queries. These embeddings help perform efficient searches within a literature database, which in this case is pubmed_chunk36.

To quickly find the most relevant documents, the system uses a FAISS index that ranks documents based on how similar their embeddings are to the queryâ€™s embedding generated by MedCPT.

The top eight documents with the best relevance scores are shown in the "Sources" section, along with their PMIDs, links, and relevance scores. The abstracts, titles, and publication dates of the three most relevant documents are then passed to the HuggingFaceTB/SmolLM2-1.7B-Instruct model, which creates a short, easy-to-read summary. For clarity, all parts of the context used to make the summary are shown with the results.

The system has a feature that shows a visual representation of the query and the documents it retrieves, by placing them in a 2D plot. This plot uses colors to show the values of the 768 different dimensions of the embeddings, helping users easily see how similar the query is to the retrieved documents, based on the MedCPT model.

The user interface, created with the Gradio library, offers an easy-to-use platform where users can type in their medical questions. It allows users to submit their queries and see the answers, including supporting information, context, and visualizations of the embeddings in a simple and clear way.

@article{jin2023medcpt,
  title={MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval},
  author={Jin, Qiao and Kim, Won and Chen, Qingyu and Comeau, Donald C and Yeganova, Lana and Wilbur, W John and Lu, Zhiyong},
  journal={Bioinformatics},
  volume={39},
  number={11},
  pages={btad651},
  year={2023},
  publisher={Oxford University Press}
}

@misc{allal2024SmolLM,
      title={SmolLM - blazingly fast and remarkably powerful}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Leandro von Werra and Thomas Wolf},
      year={2024},
}


